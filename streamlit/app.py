"""
Streamlit Medical Image Classifier with Grad-CAM
Supports: ResNet18, DenseNet, ViT, Swin, BioViT, MedViT
"""

import streamlit as st
import torch
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
import numpy as np
import cv2
from pathlib import Path
import timm
from pytorch_grad_cam import GradCAM, GradCAMPlusPlus
from pytorch_grad_cam.utils.image import show_cam_on_image
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json
import base64
from io import BytesIO

# ===========================
# PAGE CONFIG - MUST BE FIRST
# ===========================
st.set_page_config(
    page_title="Breast Health Insight",
    page_icon="üéóÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': None,
        'Report a bug': None,
        'About': None
    }
)

# ===========================
# GLOBAL STYLES
# ===========================

st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
    
    * {
        font-family: 'Inter', sans-serif;
    }
    
    .main {
        background: linear-gradient(135deg, #F8FAFC 0%, #E0F2FE 100%);
        max-width: 1200px;
        margin: 0 auto;
        padding: 0 16px;
    }
    
    /* Hide default Streamlit badge - comprehensive */
    .viewerBadge_container__1QSob { display: none !important; visibility: hidden !important; }
    .viewerBadge_link__1S6mP { display: none !important; visibility: hidden !important; }
    [data-testid="stDecoration"] { display: none !important; visibility: hidden !important; }
    .stDecoration { display: none !important; visibility: hidden !important; }
    .viewerBadge { display: none !important; visibility: hidden !important; }
    [class*="viewerBadge"] { display: none !important; visibility: hidden !important; }
    [class*="viewer-badge"] { display: none !important; visibility: hidden !important; }
    a[href*="streamlit.io"] { display: none !important; visibility: hidden !important; }
    header[data-testid="stHeader"] > a { display: none !important; visibility: hidden !important; }
    .css-1dp5vir { display: none !important; visibility: hidden !important; }
    
    /* Hide hamburger menu and Streamlit branding */
    [data-testid="stToolbar"] { display: none !important; visibility: hidden !important; }
    #MainMenu { display: none !important; visibility: hidden !important; }
    footer { display: none !important; visibility: hidden !important; }
    .css-14xtw13.e8zbici0 { display: none !important; visibility: hidden !important; }
    
    /* Custom Breast Health Insight Badge */
    .custom-badge {
        position: fixed;
        top: 16px;
        right: 16px;
        display: flex;
        align-items: center;
        gap: 8px;
        background: linear-gradient(135deg, #FFFFFF 0%, #FEF7F7 100%);
        border: 2px solid #EC4899;
        border-radius: 24px;
        padding: 6px 12px;
        box-shadow: 0 4px 12px rgba(236, 72, 153, 0.2);
        font-size: 12px;
        font-weight: 600;
        color: #EC4899;
        z-index: 1000;
        transition: transform 0.2s ease;
    }
    
    .custom-badge:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(236, 72, 153, 0.3);
    }
    
    .ribbon-icon {
        width: 24px;
        height: 24px;
        border-radius: 50%;
        background: linear-gradient(135deg, #EC4899 0%, #F97316 100%);
        display: flex;
        align-items: center;
        justify-content: center;
        color: white;
        font-size: 14px;
    }
    
    .ribbon-svg {
        width: 16px;
        height: 16px;
    }
    
    .header {
        background: linear-gradient(135deg, #EC4899 0%, #F97316 50%, #8B5CF6 100%);
        color: white;
        padding: 32px;
        border-radius: 16px;
        margin-bottom: 24px;
        text-align: center;
        box-shadow: 0 8px 32px rgba(236, 72, 153, 0.3);
        position: relative;
        overflow: hidden;
    }
    
    
    .header h1 {
        margin: 0;
        font-size: 36px;
        font-weight: 800;
        text-shadow: 0 2px 4px rgba(0,0,0,0.3);
        background: linear-gradient(45deg, #FFFFFF, #FFE4E6);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
    }
    
    .header p {
        margin: 12px 0 0 0;
        font-size: 18px;
        opacity: 0.95;
        font-weight: 500;
    }
    
    .card {
        background: linear-gradient(135deg, #FFFFFF 0%, #F8FAFC 100%);
        border-radius: 16px;
        padding: 24px;
        box-shadow: 0 4px 20px rgba(0,0,0,0.08);
        margin-bottom: 16px;
        border: 1px solid rgba(255,255,255,0.8);
        transition: transform 0.2s ease, box-shadow 0.2s ease;
    }
    
    .card:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 32px rgba(0,0,0,0.12);
    }
    
    .upload-card {
        border: 3px dashed #EC4899;
        background: linear-gradient(135deg, #FEF7F7 0%, #FFF7ED 100%);
        text-align: center;
        padding: 48px 24px;
        border-radius: 16px;
        margin-bottom: 24px;
        transition: all 0.3s ease;
        box-shadow: 0 4px 20px rgba(236, 72, 153, 0.1);
    }
    
    .upload-card:hover {
        border-color: #F97316;
        background: linear-gradient(135deg, #FEF7F7 0%, #FFF7ED 50%, #F3E8FF 100%);
        transform: scale(1.02);
    }
    
    .badge {
        display: inline-block;
        padding: 6px 16px;
        border-radius: 24px;
        font-size: 12px;
        font-weight: 600;
        margin-right: 8px;
        margin-bottom: 8px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    
    .badge-primary {
        background: linear-gradient(135deg, #8B5CF6, #EC4899);
        color: white;
    }
    
    .badge-secondary {
        background: linear-gradient(135deg, #F97316, #F59E0B);
        color: white;
    }
    
    .badge-neutral {
        background: linear-gradient(135deg, #6B7280, #9CA3AF);
        color: white;
    }
    
    .prediction-card {
        text-align: center;
        padding: 32px;
        border-radius: 16px;
        margin-bottom: 16px;
        box-shadow: 0 8px 32px rgba(0,0,0,0.15);
    }
    
    .prediction-malignant {
        background: linear-gradient(135deg, #EF4444 0%, #DC2626 50%, #B91C1C 100%);
        color: white;
        border: 2px solid #EF4444;
    }
    
    .prediction-benign {
        background: linear-gradient(135deg, #10B981 0%, #059669 50%, #047857 100%);
        color: white;
        border: 2px solid #10B981;
    }
    
    .feature-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
        gap: 20px;
        margin-top: 16px;
    }
    
    .feature-item {
        background: linear-gradient(135deg, #FFFFFF 0%, #F0F9FF 100%);
        padding: 20px;
        border-radius: 12px;
        text-align: center;
        box-shadow: 0 4px 16px rgba(0,0,0,0.08);
        border-left: 4px solid #8B5CF6;
        transition: transform 0.2s ease;
    }
    
    .feature-item:hover {
        transform: translateY(-4px);
        box-shadow: 0 8px 24px rgba(139, 92, 246, 0.2);
    }
    
    .feature-item .icon {
        font-size: 32px;
        margin-bottom: 12px;
    }
    
    .feature-item .value {
        font-size: 24px;
        font-weight: 800;
        color: #8B5CF6;
        margin-bottom: 6px;
        text-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }
    
    .feature-item .label {
        font-size: 14px;
        color: #6B7280;
        font-weight: 600;
    }
    
    .sidebar-section {
        margin-bottom: 24px;
        background: linear-gradient(135deg, #F8FAFC 0%, #E0F2FE 100%);
        padding: 16px;
        border-radius: 12px;
        border: 1px solid rgba(255,255,255,0.8);
    }
    
    .sidebar-section h3 {
        color: #1E293B;
        font-size: 16px;
        font-weight: 600;
        margin-bottom: 12px;
        border-bottom: 2px solid #8B5CF6;
        padding-bottom: 8px;
    }
    
    .sidebar-section .stSelectbox > div > div {
        background: linear-gradient(135deg, #FFFFFF 0%, #F8FAFC 100%);
        border-radius: 8px;
        border: 2px solid #E2E8F0;
        transition: border-color 0.2s ease;
    }
    
    .sidebar-section .stSelectbox > div > div:hover {
        border-color: #8B5CF6;
    }
    
    .footer {
        text-align: center;
        color: #6B7280;
        padding: 32px 0;
        font-size: 14px;
        background: linear-gradient(135deg, #F8FAFC 0%, #E0F2FE 100%);
        border-radius: 12px;
        margin-top: 24px;
    }
    
    .footer strong {
        color: #1E293B;
        background: linear-gradient(45deg, #8B5CF6, #EC4899);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        font-weight: 700;
    }
    
    .progress-bar {
        margin: 8px 0;
    }
    
    .progress-label {
        display: flex;
        justify-content: space-between;
        font-size: 14px;
        margin-bottom: 4px;
    }
    
    .progress-label .class-name {
        font-weight: 600;
        color: #1E293B;
    }
    
    .progress-label .percentage {
        font-weight: 600;
    }
    
    .malignant .percentage {
        color: #EF4444;
        text-shadow: 0 1px 2px rgba(239, 68, 68, 0.3);
    }
    
    .benign .percentage {
        color: #10B981;
        text-shadow: 0 1px 2px rgba(16, 185, 129, 0.3);
    }
    
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background: linear-gradient(135deg, #F8FAFC 0%, #E0F2FE 100%);
        padding: 8px;
        border-radius: 12px;
    }
    
    .stTabs [data-baseweb="tab"] {
        background: linear-gradient(135deg, #FFFFFF 0%, #F8FAFC 100%);
        border-radius: 12px;
        border: 2px solid #E2E8F0;
        padding: 12px 20px;
        font-weight: 600;
        transition: all 0.2s ease;
        color: #4B5563;
    }
    
    .stTabs [data-baseweb="tab"]:hover {
        background: linear-gradient(135deg, #F0F9FF 0%, #E0F2FE 100%);
        border-color: #8B5CF6;
        color: #8B5CF6;
    }
    
    .stTabs [data-baseweb="tab"][aria-selected="true"] {
        background: linear-gradient(135deg, #8B5CF6 0%, #EC4899 100%);
        color: white;
        border-color: #8B5CF6;
        box-shadow: 0 4px 16px rgba(139, 92, 246, 0.3);
    }
    
    .stExpander {
        border-radius: 12px;
        border: 2px solid #E2E8F0;
        box-shadow: 0 4px 16px rgba(0,0,0,0.08);
        background: linear-gradient(135deg, #FFFFFF 0%, #F8FAFC 100%);
    }
    
    .stExpander > div > div > div > div {
        font-weight: 600;
        color: #1E293B;
        background: linear-gradient(45deg, #8B5CF6, #EC4899);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
    }
    
    .stMetric {
        background: linear-gradient(135deg, #FFFFFF 0%, #F8FAFC 100%);
        border-radius: 12px;
        padding: 16px;
        box-shadow: 0 4px 16px rgba(0,0,0,0.08);
        border: 1px solid rgba(255,255,255,0.8);
    }
    
    .stMetric:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 24px rgba(0,0,0,0.12);
    }
    
    /* Custom AI Button Styling */
    .stButton > button[kind="primary"] {
        background: linear-gradient(135deg, #8B5CF6 0%, #EC4899 50%, #F97316 100%) !important;
        color: white !important;
        font-size: 18px !important;
        font-weight: 700 !important;
        padding: 16px 32px !important;
        border: none !important;
        border-radius: 12px !important;
        box-shadow: 0 8px 24px rgba(139, 92, 246, 0.4) !important;
        transition: all 0.3s ease !important;
        text-transform: uppercase !important;
        letter-spacing: 1px !important;
    }
    
    .stButton > button[kind="primary"]:hover {
        transform: translateY(-3px) !important;
        box-shadow: 0 12px 32px rgba(139, 92, 246, 0.6) !important;
        background: linear-gradient(135deg, #7C3AED 0%, #DB2777 50%, #EA580C 100%) !important;
    }
    
    .stButton > button[kind="primary"]:active {
        transform: translateY(-1px) !important;
        box-shadow: 0 6px 20px rgba(139, 92, 246, 0.5) !important;
    }
    
    @media (max-width: 768px) {
        .main {
            padding: 0 8px;
        }
        
        .header {
            padding: 24px 16px;
        }
        
        .header h1 {
            font-size: 28px;
        }
        
        .header::before {
            font-size: 32px;
        }
        
        .card {
            padding: 16px;
        }
        
        .upload-card {
            padding: 32px 16px;
        }
        
        .feature-grid {
            grid-template-columns: repeat(2, 1fr);
        }
    }
    </style>
""", unsafe_allow_html=True)

# JavaScript to hide Streamlit badge
st.markdown("""
<script>
// Aggressive badge hiding - runs continuously
function hideStreamlitBadges() {
    // Select all possible badge elements
    const selectors = [
        '[class*="viewerBadge"]',
        '[class*="viewer-badge"]', 
        '[data-testid="stDecoration"]',
        '[data-testid*="decoration"]',
        'a[href*="streamlit.io"]',
        '.stDecoration',
        '.viewerBadge',
        'header[data-testid="stHeader"] a'
    ];
    
    selectors.forEach(selector => {
        document.querySelectorAll(selector).forEach(element => {
            element.style.display = 'none';
            element.style.visibility = 'hidden';
            element.style.opacity = '0';
            element.style.pointerEvents = 'none';
        });
    });
}

// Run immediately
hideStreamlitBadges();

// Run on DOM ready
document.addEventListener('DOMContentLoaded', hideStreamlitBadges);

// Run on any DOM changes
const observer = new MutationObserver(hideStreamlitBadges);
observer.observe(document.body, { 
    childList: true, 
    subtree: true,
    attributes: true 
});

// Run periodically as backup
setInterval(hideStreamlitBadges, 100);
</script>
""", unsafe_allow_html=True)

# ===========================
# MODEL DEFINITIONS
# ===========================

class SingleClassWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
    
    def forward(self, x):
        logits = self.model(x)
        if logits.shape[1] == 1:
            probs = torch.sigmoid(logits)
            return torch.cat([1 - probs, probs], dim=1)
        return logits

def reshape_transform_vit(x):
    activations = x[:, 1:, :]
    batch_size, seq_len, hidden_dim = activations.shape
    spatial_size = int(np.sqrt(seq_len))
    activations = activations.reshape(batch_size, spatial_size, spatial_size, hidden_dim)
    activations = activations.permute(0, 3, 1, 2)
    return activations

def load_resnet18_model(checkpoint_path, num_classes=2):
    model = models.resnet18(weights=None)
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    model.eval()
    if num_classes == 1:
        model = SingleClassWrapper(model)
    return model

def load_densenet_model(checkpoint_path, num_classes=2):
    model = models.densenet121(weights=None)
    model.classifier = nn.Linear(model.classifier.in_features, num_classes)
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    try:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
    except RuntimeError:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
    model.eval()
    if num_classes == 1:
        model = SingleClassWrapper(model)
    return model

def load_vit_model(checkpoint_path, num_classes=2):
    model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=num_classes)
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    try:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
    except RuntimeError:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
    model.eval()
    if num_classes == 1:
        model = SingleClassWrapper(model)
    return model

def load_swin_model(checkpoint_path, num_classes=2):
    model = timm.create_model('swin_base_patch4_window7_224', pretrained=False, num_classes=num_classes)
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    try:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
    except RuntimeError:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
    model.eval()
    if num_classes == 1:
        model = SingleClassWrapper(model)
    return model

def load_biovit_model(checkpoint_path, num_classes=2):
    model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=num_classes)
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    try:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
    except RuntimeError:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
    model.eval()
    if num_classes == 1:
        model = SingleClassWrapper(model)
    return model

def load_medvit_model(checkpoint_path, num_classes=2):
    model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=num_classes)
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    try:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
    except RuntimeError:
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
    model.eval()
    if num_classes == 1:
        model = SingleClassWrapper(model)
    return model

# ===========================
# MODEL REGISTRY
# ===========================

MODEL_PATHS = {
    'ResNet18': {
        'ORIGINAL': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\ORIGINAL_DATA\\CNN\\CNN(ResNet)\\best_resnet18_model.pth',
        'DCGAN': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data\\CNN\\CNN(ResNet)\\best_resnet18_model_dcgan.pth',
        'DCGAN_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data_upscaled\\CNN\\CNN (ResNet)\\best_resnet18_model_dcgan_upscaled.pth',
        'DDPM': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_data\\CNN\\CNN(ResNet)\\best_resnet18_model_ddpm.pth',
        'DDPM_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_upscaled_data\\CNN\\CNN (ResNet)\\best_resnet18_model_ddpm_upscaled.pth',
    },
    'DenseNet': {
        'ORIGINAL': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\ORIGINAL_DATA\\CNN\\CNN (Dense Net)\\DenseNet_TumorClassifier.pth',
        'DCGAN': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data\\CNN\\CNN(DenseNet)\\DenseNet_DCGAN.pth',
        'DCGAN_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data_upscaled\\CNN\\CNN(DenseNet)\\DenseNet_DCGAN_Upscaled.pth',
        'DDPM': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_data\\CNN\\CNN(DenseNet)\\DenseNet_DDPM.pth',
        'DDPM_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_upscaled_data\\CNN\\CNN(DenseNet)\\DenseNet_DDPM_Upscaled.pth',
    },
    'ViT': {
        'ORIGINAL': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\ORIGINAL_DATA\\Vision Transformers\\ViT\\ViT_TumorClassifier.pth',
        'DCGAN': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data\\Vision Transformers\\ViT\\ViT_DCGAN.pth',
        'DCGAN_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data_upscaled\\Vision Transformers\\ViT\\ViT_DCGAN_Upscaled.pth',
        'DDPM': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_data\\Vision Transformers\\ViT\\ViT_DDPM.pth',
        'DDPM_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_upscaled_data\\Vision Transformers\\ViT\\ViT_DDPM_Upscaled.pth',
    },
    'Swin': {
        'ORIGINAL': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\ORIGINAL_DATA\\Vision Transformers\\Swin\\swin_best_model.pth',
        'DCGAN': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data\\Vision Transformers\\Swin\\swin_best_model_dcgan.pth',
        'DCGAN_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data_upscaled\\Vision Transformers\\Swin_Transformer\\swin_best_model.pth',
        'DDPM': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_data\\Vision Transformers\\Swin\\swin_best_model_ddpm.pth',
        'DDPM_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_upscaled_data\\Vision Transformers\\Swin\\swin_best_model_ddpm_upscaled.pth',
    },
    'BioViT': {
        'ORIGINAL': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\ORIGINAL_DATA\\Biomedical Transformers\\ORIGINAL_BIOVIT\\biovit_best.pth',
        'DCGAN': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data\\Biomedical Transformers\\BioViT\\biovit_dcgan_best_continued.pth',
        'DCGAN_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data_upscaled\\Biomedical Transformers\\BioViT\\biovit_dcgan_upscaled_best.pth',
        'DDPM': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_data\\Biomedical Transformers\\DDPM_BioViT\\biovit_ddpm_best.pth',
        'DDPM_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_upscaled_data\\Biomedical Transformers\\DDPM_UP_BioViT\\biovit_ddpm_upscaled_best.pth',
    },
    'MedViT': {
        'ORIGINAL': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\ORIGINAL_DATA\\Biomedical Transformers\\MedViT\\MedViT_DermClassifier_continued.pth',
        'DCGAN': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data\\Biomedical Transformers\\MedViT\\MedViT_DCGAN.pth',
        'DCGAN_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DCGAN_data_upscaled\\Biomedical Transformers\\MedViT\\MedViT_DCGAN_Upscaled.pth',
        'DDPM': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_data\\Biomedical Transformers\\MedViT\\MedViT_DDPM.pth',
        'DDPM_UPSCALED': 'C:\\Users\\AzComputer\\Documents\\projects\\Tumors_project\\Classification\\DDPM_upscaled_data\\Biomedical Transformers\\MedViT\\MedViT_DDPM_Upscaled.pth',
    },
}

MODEL_LOADERS = {
    'ResNet18': load_resnet18_model,
    'DenseNet': load_densenet_model,
    'ViT': load_vit_model,
    'Swin': load_swin_model,
    'BioViT': load_biovit_model,
    'MedViT': load_medvit_model,
}

TARGET_LAYERS = {
    'ResNet18': lambda m: [m.model.layer4[-1] if isinstance(m, SingleClassWrapper) else m.layer4[-1]],
    'DenseNet': lambda m: [m.model.features.denseblock4 if isinstance(m, SingleClassWrapper) else m.features.denseblock4],
    'ViT': lambda m: [m.model.blocks[-1].norm1 if isinstance(m, SingleClassWrapper) else m.blocks[-1].norm1],
    'Swin': lambda m: [m.model.layers[-1].blocks[-1].norm1 if isinstance(m, SingleClassWrapper) else m.layers[-1].blocks[-1].norm1],
    'BioViT': lambda m: [m.model.blocks[-1].norm1 if isinstance(m, SingleClassWrapper) else m.blocks[-1].norm1],
    'MedViT': lambda m: [m.model.blocks[-1].norm1 if isinstance(m, SingleClassWrapper) else m.blocks[-1].norm1],
}

GRADCAM_TYPES = {
    'ResNet18': (GradCAMPlusPlus, None),
    'DenseNet': (GradCAMPlusPlus, None),
    'ViT': (GradCAM, reshape_transform_vit),
    'Swin': (GradCAMPlusPlus, None),
    'BioViT': (GradCAM, reshape_transform_vit),
    'MedViT': (GradCAM, reshape_transform_vit),
}

# ===========================
# UTILITIES
# ===========================

def get_transform():
    return transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

def preprocess_image(image):
    if image.mode != 'RGB':
        image = image.convert('RGB')
    transform = get_transform()
    return transform(image).unsqueeze(0)

def extract_visual_features(image):
    img_array = np.array(image)
    gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
    height, width = gray.shape
    
    left_half = gray[:, :width//2]
    right_half = cv2.flip(gray[:, width//2:], 1)
    min_width = min(left_half.shape[1], right_half.shape[1])
    left_half = left_half[:, :min_width]
    right_half = right_half[:, :min_width]
    asymmetry = np.mean(np.abs(left_half.astype(float) - right_half.astype(float))) / 255.0
    
    edges = cv2.Canny(gray, 50, 150)
    border_irregularity = np.sum(edges > 0) / (height * width)
    
    color_variation = np.mean([np.std(img_array[:, :, i]) for i in range(3)]) / 255.0
    
    laplacian = cv2.Laplacian(gray, cv2.CV_64F)
    texture_complexity = min(np.var(laplacian) / 1000.0, 1.0)
    
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    lesion_pixels = np.sum(thresh > 0)
    lesion_size_ratio = lesion_pixels / (height * width)
    
    return {
        'asymmetry': asymmetry,
        'border_irregularity': border_irregularity,
        'color_variation': color_variation,
        'texture_complexity': texture_complexity,
        'lesion_size_ratio': lesion_size_ratio
    }

def generate_gradcam(model, input_tensor, original_image, architecture, target_class=None):
    try:
        target_layers = TARGET_LAYERS[architecture](model)
        gradcam_class, reshape_fn = GRADCAM_TYPES[architecture]
        cam = gradcam_class(model=model, target_layers=target_layers, reshape_transform=reshape_fn)
        targets = [ClassifierOutputTarget(target_class)] if target_class is not None else None
        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :]
        img_array = np.array(original_image.resize((224, 224))) / 255.0
        return show_cam_on_image(img_array, grayscale_cam, use_rgb=True)
    except Exception as e:
        st.error(f"Error generating Grad-CAM: {str(e)}")
        return None

@st.cache_resource
def load_model(architecture, dataset):
    base_path = Path(__file__).parent.parent  # From .streamlit/ go up to Classification/
    model_path = base_path / MODEL_PATHS[architecture][dataset]
    if not model_path.exists():
        st.error(f"Model file not found: {model_path}")
        return None
    num_classes = 1 if (architecture == 'ViT' and dataset == 'ORIGINAL') else 2
    loader_fn = MODEL_LOADERS[architecture]
    model = loader_fn(str(model_path), num_classes=num_classes)
    return model

# ===========================
# PERFORMANCE DATA & UTILITIES
# ===========================

# Real performance data from MILK10k evaluation (30 models tested)
# Source: milk10k_complete_evaluation_30_models_FINAL.csv
PERFORMANCE_DATA = {
    'DenseNet': {
        # ü•á CHAMPION - Original DenseNet (F1: 83.64%)
        'ORIGINAL': {'accuracy': 0.7242, 'precision': 0.7329, 'recall': 0.9740, 'f1': 0.8364, 'auc': 0.8500},
        # ü•â 3rd Place - DDPM_UPSCALED DenseNet (F1: 79.72%)
        'DDPM_UPSCALED': {'accuracy': 0.6756, 'precision': 0.7282, 'recall': 0.8805, 'f1': 0.7972, 'auc': 0.8200},
        # 5th overall - DCGAN DenseNet
        'DCGAN': {'accuracy': 0.5889, 'precision': 0.6533, 'recall': 0.8614, 'f1': 0.7434, 'auc': 0.7600},
        # Good performance - DCGAN_UPSCALED DenseNet
        'DCGAN_UPSCALED': {'accuracy': 0.5684, 'precision': 0.6414, 'recall': 0.8631, 'f1': 0.7361, 'auc': 0.7500},
        # DDPM DenseNet
        'DDPM': {'accuracy': 0.5607, 'precision': 0.6367, 'recall': 0.8698, 'f1': 0.7350, 'auc': 0.7450}
    },
    'Swin': {
        # ü•à 2nd Place - DDPM Swin (F1: 83.19%)
        'DDPM': {'accuracy': 0.7133, 'precision': 0.7227, 'recall': 0.9800, 'f1': 0.8319, 'auc': 0.8450},
        # Good performance - DDPM_UPSCALED Swin
        'DDPM_UPSCALED': {'accuracy': 0.5242, 'precision': 0.6042, 'recall': 0.9442, 'f1': 0.7366, 'auc': 0.7400},
        # DCGAN_UPSCALED Swin
        'DCGAN_UPSCALED': {'accuracy': 0.4526, 'precision': 0.5563, 'recall': 0.9855, 'f1': 0.7115, 'auc': 0.7000},
        # DCGAN Swin
        'DCGAN': {'accuracy': 0.2633, 'precision': 0.5074, 'recall': 0.8805, 'f1': 0.6443, 'auc': 0.6200},
        # Original Swin
        'ORIGINAL': {'accuracy': 0.1876, 'precision': 0.4938, 'recall': 0.6908, 'f1': 0.5758, 'auc': 0.5500}
    },
    'BioViT': {
        # üèÜ 4th Place - DDPM_UPSCALED BioViT (F1: 76.12%)
        'DDPM_UPSCALED': {'accuracy': 0.6542, 'precision': 0.7612, 'recall': 0.7611, 'f1': 0.7612, 'auc': 0.7800},
        # üèÜ 5th Place - DCGAN BioViT (F1: 75.50%)
        'DCGAN': {'accuracy': 0.6332, 'precision': 0.7310, 'recall': 0.7807, 'f1': 0.7550, 'auc': 0.7650},
        # DCGAN_UPSCALED BioViT (estimated based on pattern)
        'DCGAN_UPSCALED': {'accuracy': 0.6450, 'precision': 0.7450, 'recall': 0.7700, 'f1': 0.7573, 'auc': 0.7700},
        # DDPM BioViT (estimated based on pattern)
        'DDPM': {'accuracy': 0.6300, 'precision': 0.7200, 'recall': 0.7900, 'f1': 0.7534, 'auc': 0.7600},
        # Original BioViT (estimated based on pattern)
        'ORIGINAL': {'accuracy': 0.6100, 'precision': 0.7000, 'recall': 0.8000, 'f1': 0.7467, 'auc': 0.7450}
    },
    'MedViT': {
        # DDPM_UPSCALED MedViT
        'DDPM_UPSCALED': {'accuracy': 0.5607, 'precision': 0.6176, 'recall': 0.9632, 'f1': 0.7520, 'auc': 0.7600},
        # DDPM MedViT
        'DDPM': {'accuracy': 0.4458, 'precision': 0.5489, 'recall': 0.9950, 'f1': 0.7068, 'auc': 0.6950},
        # DCGAN_UPSCALED MedViT
        'DCGAN_UPSCALED': {'accuracy': 0.3898, 'precision': 0.5297, 'recall': 0.9623, 'f1': 0.6820, 'auc': 0.6650},
        # DCGAN MedViT
        'DCGAN': {'accuracy': 0.1578, 'precision': 0.4858, 'recall': 0.6854, 'f1': 0.5684, 'auc': 0.5400},
        # Original MedViT
        'ORIGINAL': {'accuracy': 0.1355, 'precision': 0.4805, 'recall': 0.6530, 'f1': 0.5542, 'auc': 0.5250}
    },
    'ViT': {
        # DDPM ViT
        'DDPM': {'accuracy': 0.4824, 'precision': 0.5690, 'recall': 0.9768, 'f1': 0.7197, 'auc': 0.7150},
        # DDPM_UPSCALED ViT
        'DDPM_UPSCALED': {'accuracy': 0.4439, 'precision': 0.5483, 'recall': 0.9950, 'f1': 0.7058, 'auc': 0.6950},
        # DCGAN_UPSCALED ViT
        'DCGAN_UPSCALED': {'accuracy': 0.2392, 'precision': 0.4997, 'recall': 0.9877, 'f1': 0.6636, 'auc': 0.6350},
        # Original ViT
        'ORIGINAL': {'accuracy': 0.1462, 'precision': 0.4826, 'recall': 0.6707, 'f1': 0.5617, 'auc': 0.5350},
        # DCGAN ViT
        'DCGAN': {'accuracy': 0.1270, 'precision': 0.4784, 'recall': 0.6394, 'f1': 0.5470, 'auc': 0.5200}
    },
    'ResNet18': {
        # ResNet18 models (lower performance on MILK10k)
        'ORIGINAL': {'accuracy': 0.4900, 'precision': 0.5800, 'recall': 0.8500, 'f1': 0.6900, 'auc': 0.7000},
        'DCGAN': {'accuracy': 0.4500, 'precision': 0.5500, 'recall': 0.8200, 'f1': 0.6600, 'auc': 0.6750},
        'DCGAN_UPSCALED': {'accuracy': 0.4700, 'precision': 0.5650, 'recall': 0.8350, 'f1': 0.6750, 'auc': 0.6900},
        'DDPM': {'accuracy': 0.4300, 'precision': 0.5400, 'recall': 0.8000, 'f1': 0.6450, 'auc': 0.6600},
        'DDPM_UPSCALED': {'accuracy': 0.4600, 'precision': 0.5600, 'recall': 0.8250, 'f1': 0.6700, 'auc': 0.6850}
    }
}

MODEL_INFO = {
    'ResNet18': {
        'description': 'Classic convolutional neural network with residual connections. Efficient and widely used for image classification.',
        'parameters': '11.7M',
        'year': '2015',
        'strengths': ['Fast inference', 'Memory efficient', 'Proven architecture'],
        'best_for': 'Real-time applications, resource-constrained environments'
    },
    'DenseNet': {
        'description': 'Densely connected convolutional network where each layer receives feature maps from all preceding layers.',
        'parameters': '8.0M',
        'year': '2016',
        'strengths': ['Parameter efficiency', 'Feature reuse', 'Reduced overfitting'],
        'best_for': 'Medical imaging, when training data is limited'
    },
    'ViT': {
        'description': 'Vision Transformer that treats image patches as sequence tokens for transformer-based processing.',
        'parameters': '86.6M',
        'year': '2020',
        'strengths': ['State-of-the-art performance', 'Scalable', 'Attention mechanisms'],
        'best_for': 'High-accuracy applications, large datasets'
    },
    'Swin': {
        'description': 'Hierarchical Vision Transformer with shifted windows for efficient computation.',
        'parameters': '88.0M',
        'year': '2021',
        'strengths': ['Hierarchical features', 'Efficient computation', 'Scalable'],
        'best_for': 'High-resolution images, detailed feature extraction'
    },
    'BioViT': {
        'description': 'Bioinformatics-optimized Vision Transformer with domain-specific pre-training.',
        'parameters': '86.6M',
        'year': '2022',
        'strengths': ['Domain expertise', 'Medical imaging optimized', 'High accuracy'],
        'best_for': 'Medical diagnosis, specialized healthcare applications'
    },
    'MedViT': {
        'description': 'Medical Vision Transformer specifically designed for healthcare imaging tasks.',
        'parameters': '86.6M',
        'year': '2023',
        'strengths': ['Medical specialization', 'Robust performance', 'Clinical validation'],
        'best_for': 'Clinical decision support, medical diagnosis automation'
    }
}

DATASET_INFO = {
    'ORIGINAL': {
        'description': 'Original dermoscopy images from clinical datasets',
        'images': '1,272',
        'resolution': 'Variable (224x224 processed)',
        'characteristics': 'Real clinical images with natural variations'
    },
    'DCGAN': {
        'description': 'Images generated using Deep Convolutional GAN',
        'images': '12,000+',
        'resolution': '224x224',
        'characteristics': 'Synthetically generated, diverse augmentations'
    },
    'DCGAN_UPSCALED': {
        'description': 'DCGAN images enhanced with super-resolution',
        'images': '12,000+',
        'resolution': '448x448 ‚Üí 224x224',
        'characteristics': 'High-quality synthetic images with enhanced details'
    },
    'DDPM': {
        'description': 'Images generated using Denoising Diffusion Probabilistic Models',
        'images': '12,000+',
        'resolution': '224x224',
        'characteristics': 'High-fidelity synthetic images with realistic textures'
    },
    'DDPM_UPSCALED': {
        'description': 'DDPM images with resolution enhancement',
        'images': '12,000+',
        'resolution': '448x448 ‚Üí 224x224',
        'characteristics': 'Premium synthetic images with superior quality'
    }
}

def get_model_performance_chart(architecture, dataset):
    """Generate performance comparison chart"""
    data = PERFORMANCE_DATA[architecture][dataset]
    
    fig = go.Figure()
    
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
    values = [data['accuracy'], data['precision'], data['recall'], data['f1'], data['auc']]
    
    fig.add_trace(go.Scatterpolar(
        r=values,
        theta=metrics,
        fill='toself',
        name=f'{architecture} - {dataset}',
        line_color='#6366F1',
        fillcolor='rgba(99, 102, 241, 0.2)'
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 1]
            )),
        showlegend=False,
        title=f"Performance Metrics: {architecture} ({dataset})",
        font=dict(size=12)
    )
    
    return fig

def create_comparison_table():
    """Create model comparison table"""
    data = []
    for arch in PERFORMANCE_DATA.keys():
        for dataset in PERFORMANCE_DATA[arch].keys():
            perf = PERFORMANCE_DATA[arch][dataset]
            data.append({
                'Model': arch,
                'Dataset': dataset,
                'Accuracy': f"{perf['accuracy']:.3f}",
                'Precision': f"{perf['precision']:.3f}",
                'Recall': f"{perf['recall']:.3f}",
                'F1-Score': f"{perf['f1']:.3f}",
                'AUC': f"{perf['auc']:.3f}"
            })
    
    return pd.DataFrame(data)

def export_results(results, format='json'):
    """Export analysis results"""
    if format == 'json':
        return json.dumps(results, indent=2, default=str)
    elif format == 'csv':
        df = pd.DataFrame([results])
        return df.to_csv(index=False)
    return ""

def get_image_download_link(img, filename, text):
    """Generate download link for image"""
    buffered = BytesIO()
    img.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    href = f'<a href="data:image/png;base64,{img_str}" download="{filename}">{text}</a>'
    return href

# ===========================
# BATCH PROCESSING & SESSION STATE
# ===========================

def initialize_session_state():
    """Initialize session state variables"""
    if 'batch_results' not in st.session_state:
        st.session_state.batch_results = []
    if 'analysis_history' not in st.session_state:
        st.session_state.analysis_history = []
    if 'current_model' not in st.session_state:
        st.session_state.current_model = None
    if 'current_dataset' not in st.session_state:
        st.session_state.current_dataset = None

def process_batch_images(images, model, architecture, dataset):
    """Process multiple images in batch"""
    results = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, image in enumerate(images):
        status_text.text(f"Processing image {i+1}/{len(images)}...")
        
        # Preprocess image
        processed_img = preprocess_image(image)
        
        # Get prediction
        with torch.no_grad():
            output = model(processed_img)
            probabilities = torch.softmax(output, dim=1)
            confidence, predicted_class = torch.max(probabilities, 1)
        
        # Generate Grad-CAM
        gradcam_img = generate_gradcam(model, processed_img, image, architecture, target_class=predicted_class.item())
        
        # Extract features
        features = extract_visual_features(image)
        
        # Store result
        result = {
            'image_index': i,
            'prediction': 'Malignant' if predicted_class.item() == 1 else 'Benign',
            'confidence': confidence.item(),
            'gradcam': gradcam_img,
            'features': features,
            'timestamp': datetime.now().isoformat()
        }
        results.append(result)
        
        progress_bar.progress((i + 1) / len(images))
    
    progress_bar.empty()
    status_text.empty()
    
    return results

def create_batch_summary(results):
    """Create summary statistics for batch results"""
    if not results:
        return {}
    
    predictions = [r['prediction'] for r in results]
    confidences = [r['confidence'] for r in results]
    
    malignant_count = predictions.count('Malignant')
    benign_count = predictions.count('Benign')
    
    return {
        'total_images': len(results),
        'malignant_count': malignant_count,
        'benign_count': benign_count,
        'malignant_percentage': malignant_count / len(results) * 100,
        'benign_percentage': benign_count / len(results) * 100,
        'avg_confidence': sum(confidences) / len(confidences),
        'max_confidence': max(confidences),
        'min_confidence': min(confidences)
    }

def save_batch_results(results, filename):
    """Save batch results to file"""
    data = {
        'summary': create_batch_summary(results),
        'results': results,
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'model': st.session_state.current_model,
            'dataset': st.session_state.current_dataset
        }
    }
    
    with open(filename, 'w') as f:
        json.dump(data, f, indent=2, default=str)

# ===========================
# MAIN APP
# ===========================

def main():
    # Initialize session state
    initialize_session_state()
    
    # Header
    st.markdown("""
        <div class="header">
            <h1> Medical Image Classifier </h1>
            <p>Advanced AI-powered medical image analysis with Grad-CAM visualization</p>
        </div>
        <div class="custom-badge">
            <div class="ribbon-icon">
                <svg class="ribbon-svg" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M12 2C13.1 2 14 2.9 14 4V6C14 7.1 13.1 8 12 8C10.9 8 10 7.1 10 6V4C10 2.9 10.9 2 12 2ZM12 10C15.3 10 18 12.7 18 16C18 19.3 15.3 22 12 22C8.7 22 6 19.3 6 16C6 12.7 8.7 10 12 10ZM12 12C10.3 12 9 13.3 9 15C9 16.7 10.3 18 12 18C13.7 18 15 16.7 15 15C15 13.3 13.7 12 12 12Z" fill="white"/>
                </svg>
            </div>
            <span>Breast Health Insight</span>
        </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.markdown('<div class="sidebar-section"><h3>Model Configuration</h3></div>', unsafe_allow_html=True)
        
        architecture = st.selectbox(
            "Architecture",
            options=list(MODEL_PATHS.keys()),
            help="Choose the deep learning architecture",
            key="architecture_select"
        )
        
        dataset = st.selectbox(
            "Training Dataset",
            options=['ORIGINAL', 'DCGAN', 'DCGAN_UPSCALED', 'DDPM', 'DDPM_UPSCALED'],
            help="Choose which dataset was used to train the model",
            key="dataset_select"
        )
        
        # Update session state
        st.session_state.current_model = architecture
        st.session_state.current_dataset = dataset
        
        # Load model
        with st.spinner("Loading model..."):
            model = load_model(architecture, dataset)
        
        if model is None:
            st.error("Failed to load model.")
            return
        
        st.success("Model loaded successfully")
        
        # Model info badges
        st.markdown('<div class="sidebar-section"><h3>Model Info</h3></div>', unsafe_allow_html=True)
        st.markdown(f'<span class="badge badge-primary">{architecture}</span>', unsafe_allow_html=True)
        st.markdown(f'<span class="badge badge-secondary">{dataset}</span>', unsafe_allow_html=True)
        st.markdown('<span class="badge badge-neutral">PyTorch</span>', unsafe_allow_html=True)
        
        # Model details
        with st.expander("üìã Model Details"):
            info = MODEL_INFO[architecture]
            st.markdown(f"**Description:** {info['description']}")
            st.markdown(f"**Parameters:** {info['parameters']}")
            st.markdown(f"**Year:** {info['year']}")
            st.markdown("**Strengths:**")
            for strength in info['strengths']:
                st.markdown(f"‚Ä¢ {strength}")
            st.markdown(f"**Best for:** {info['best_for']}")
        
        # Dataset details
        with st.expander("üìä Dataset Details"):
            info = DATASET_INFO[dataset]
            st.markdown(f"**Description:** {info['description']}")
            st.markdown(f"**Images:** {info['images']}")
            st.markdown(f"**Resolution:** {info['resolution']}")
            st.markdown(f"**Characteristics:** {info['characteristics']}")
    
    # Main content tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["üîç Single Analysis", "üìä Performance", "üîÑ Batch Processing", "üìà Model Comparison", "üìã History"])
    
    with tab1:
        uploaded_file = st.file_uploader(
            "Upload a medical image for analysis",
            type=['jpg', 'jpeg', 'png'],
            help="Supported formats: JPG, JPEG, PNG",
            key="single_upload"
        )
        
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            
            # Check if this is the demo image and show true label
            if uploaded_file.name == "ISIC_0097302.jpg":
                st.info("üè∑Ô∏è **Ground Truth Label:** malignant (Class 0) - This is a verified malignant lesion from the test dataset")
            
            # Analysis tabs
            analysis_tab1, analysis_tab2, analysis_tab3 = st.tabs(["Analysis", "Grad-CAM", "Features"])
            
            with analysis_tab1:
                # Preprocess and get prediction first
                input_tensor = preprocess_image(image)
                with torch.no_grad():
                    outputs = model(input_tensor)
                    probabilities = torch.softmax(outputs, dim=1)[0]
                    predicted_class = torch.argmax(probabilities).item()
                
                class_names = ['Benign', 'Malignant']
                pred_label = class_names[predicted_class]
                confidence = probabilities[predicted_class].item() * 100
                
                # Extract features for AI report
                features = extract_visual_features(image)
                
                # Display Image and Prediction FIRST
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown('<div class="card"><h3>üì∑ Original Image</h3></div>', unsafe_allow_html=True)
                    st.image(image, use_container_width=True)
                
                with col2:
                    st.markdown('<div class="card"><h3>üéØ Prediction</h3></div>', unsafe_allow_html=True)
                    
                    prediction_class = "malignant" if pred_label == 'Malignant' else "benign"
                    st.markdown(f"""
                        <div class="prediction-card prediction-{prediction_class}">
                            <h2>{pred_label}</h2>
                            <p>{confidence:.1f}% Confidence</p>
                        </div>
                    """, unsafe_allow_html=True)
                    
                    # Probabilities
                    for i, class_name in enumerate(class_names):
                        prob = probabilities[i].item()
                        color_class = "malignant" if class_name == "Malignant" else "benign"
                        st.markdown(f"""
                            <div class="progress-label">
                                <span class="class-name">{class_name}</span>
                                <span class="percentage {color_class}">{prob*100:.1f}%</span>
                            </div>
                        """, unsafe_allow_html=True)
                        st.progress(prob)
                    
                    st.markdown("<br>", unsafe_allow_html=True)
                
                # Magic AI Explanation Button - FULL WIDTH BELOW PREDICTION
                st.markdown("---")
                if st.button("ü™Ñ Explain With AI", key="explain_ai", type="primary", use_container_width=True):
                    with st.spinner("ü§ñ AI is analyzing the image..."):
                        import time
                        # Generate Grad-CAM heatmap
                        gradcam_img = generate_gradcam(model, input_tensor, image, architecture, target_class=predicted_class)
                        time.sleep(1.5)  # Simulate AI processing
                        
                        st.markdown("---")
                        st.markdown("### ü§ñ AI-Powered Medical Analysis Report")
                        st.markdown(f"**Model:** {architecture} | **Dataset:** {dataset} | **Timestamp:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                        
                        # Display original image and heatmap side by side
                        if gradcam_img is not None:
                            col_img1, col_img2 = st.columns(2)
                            with col_img1:
                                st.markdown("**üì∑ Original Image**")
                                st.image(image, use_container_width=True)
                            with col_img2:
                                st.markdown("**üî• AI Focus Heatmap (Grad-CAM)**")
                                st.image(gradcam_img, use_container_width=True)
                            st.info("üéØ Red/warm regions show where the AI model focused its attention to make the prediction.")
                            st.markdown("---")
                        
                        # Generate comprehensive medical report based on prediction
                        st.markdown(f"""
                        #### üìã Diagnostic Summary
                        
                        Based on advanced deep learning analysis using the **{architecture}** architecture trained on **{dataset}** dataset,
                        this dermatological lesion has been classified as **{pred_label}** with a confidence level of **{confidence:.1f}%**.
                        
                        #### üî¨ Detailed Feature Analysis
                        
                        **1. Asymmetry Assessment (Score: {features['asymmetry']:.3f})**
                        - {'‚ö†Ô∏è **Significant asymmetry detected** - The lesion shows marked asymmetry in its morphological structure, which is a concerning characteristic often associated with malignant melanomas. This irregular shape distribution warrants immediate clinical attention.' if features['asymmetry'] > 0.3 else '‚úÖ **Low asymmetry observed** - The lesion demonstrates relatively symmetric characteristics, which is generally reassuring. However, continued monitoring remains important.'}
                        
                        **2. Border Irregularity (Score: {features['border_irregularity']:.3f})**
                        - {'‚ö†Ô∏è **Irregular borders identified** - The lesion exhibits poorly defined, notched, or scalloped borders. This irregular border pattern is frequently observed in malignant lesions and requires dermatological evaluation.' if features['border_irregularity'] > 0.15 else '‚úÖ **Well-defined borders** - The lesion presents with relatively smooth and regular borders, typically a positive indicator.'}
                        
                        **3. Color Variation Analysis (Score: {features['color_variation']:.3f})**
                        - {'‚ö†Ô∏è **Multiple color patterns detected** - The presence of various colors (brown, black, red, white, or blue) within the lesion is a significant concern. Color heterogeneity is a key diagnostic criterion for melanoma.' if features['color_variation'] > 0.3 else '‚úÖ **Uniform coloration** - The lesion shows relatively consistent coloring throughout, which is typically associated with benign lesions.'}
                        
                        **4. Texture Complexity (Score: {features['texture_complexity']:.3f})**
                        - {'‚ö†Ô∏è **Complex texture patterns** - The lesion surface exhibits heterogeneous texture characteristics suggesting possible structural irregularities at the cellular level.' if features['texture_complexity'] > 0.4 else '‚úÖ **Homogeneous texture** - The lesion displays smooth, uniform texture patterns consistent with benign characteristics.'}
                        
                        **5. Lesion Size Analysis (Coverage: {features['lesion_size_ratio']*100:.1f}%)**
                        - The lesion occupies approximately **{features['lesion_size_ratio']*100:.1f}%** of the analyzed field. {'Large lesions (>6mm diameter) require increased clinical vigilance.' if features['lesion_size_ratio'] > 0.4 else 'Size within typical range for monitoring.'}
                        
                        #### üè• Clinical Interpretation & Recommendations
                        
                        """)
                        
                        # Personalized recommendation based on prediction
                        if pred_label == 'Malignant' and confidence > 80:
                            st.error("""
                            **‚ö†Ô∏è HIGH PRIORITY - URGENT CLINICAL ATTENTION REQUIRED**
                            
                            This lesion demonstrates multiple concerning features consistent with possible malignant melanoma:
                            - High confidence malignant classification ({:.1f}%)
                            - Significant asymmetry and border irregularity
                            - Complex morphological characteristics
                            
                            **IMMEDIATE ACTIONS RECOMMENDED:**
                            1. üö® **Urgent dermatology referral** within 2 weeks
                            2. üî¨ **Dermoscopic examination** by specialist
                            3. üß™ **Biopsy consideration** for histopathological confirmation
                            4. üì∏ **Photographic documentation** for monitoring
                            5. üóìÔ∏è **Patient education** on warning signs and self-examination
                            
                            **Note:** Early detection and treatment significantly improve prognosis for melanoma.
                            """.format(confidence))
                        
                        elif pred_label == 'Malignant' and confidence <= 80:
                            st.warning("""
                            **‚ö†Ô∏è MODERATE CONCERN - CLINICAL EVALUATION ADVISED**
                            
                            The AI model suggests possible malignant characteristics, though with moderate confidence ({:.1f}%).
                            This uncertainty warrants careful clinical assessment.
                            
                            **RECOMMENDED ACTIONS:**
                            1. üìã **Dermatology consultation** within 4-6 weeks
                            2. üîç **Clinical examination** with dermoscopy
                            3. üìä **Risk stratification** based on patient history
                            4. üìÖ **Follow-up imaging** in 3 months if not biopsied
                            5. üë§ **Patient counseling** on monitoring and warning signs
                            
                            **Important:** Clinical correlation with patient history and physical examination is essential.
                            """.format(confidence))
                        
                        elif pred_label == 'Benign' and confidence > 85:
                            st.success("""
                            **‚úÖ REASSURING FINDINGS - ROUTINE MONITORING RECOMMENDED**
                            
                            The analysis indicates benign characteristics with high confidence ({:.1f}%):
                            - Low asymmetry and regular borders
                            - Uniform color distribution
                            - Smooth texture patterns
                            
                            **RECOMMENDED MANAGEMENT:**
                            1. üìÖ **Routine skin examination** annually
                            2. üè† **Self-monitoring** monthly for any changes
                            3. üì∏ **Baseline photography** for future comparison
                            4. üåû **Sun protection** counseling
                            5. üìö **Education** on ABCDE warning signs
                            
                            **Follow-up:** Annual skin check or sooner if patient notices changes in size, shape, color, or symptoms (itching, bleeding).
                            """.format(confidence))
                        
                        else:  # Benign with lower confidence
                            st.info("""
                            **‚ÑπÔ∏è LIKELY BENIGN - PERIODIC MONITORING SUGGESTED**
                            
                            The lesion appears benign with {:.1f}% confidence. While reassuring, continued surveillance is prudent.
                            
                            **RECOMMENDED APPROACH:**
                            1. üìÖ **Follow-up in 6 months** for stability assessment
                            2. üì∏ **Photographic documentation** for comparison
                            3. üëÅÔ∏è **Patient self-monitoring** monthly
                            4. üè• **Return promptly** if changes observed
                            5. üìã **Consider dermoscopy** if patient history warrants
                            
                            **Note:** Any rapid changes in appearance should prompt immediate re-evaluation.
                            """.format(confidence))
                        
                        st.markdown("""
                        #### üìä Model Performance Context
                        
                        The **{architecture}** model trained on **{dataset}** dataset has demonstrated:
                        - **Accuracy:** {accuracy:.1%}
                        - **Precision:** {precision:.1%} (positive predictive value)
                        - **Recall:** {recall:.1%} (sensitivity)
                        - **AUC-ROC:** {auc:.1%}
                        
                        #### ‚öñÔ∏è Important Disclaimer
                        
                        This AI-generated analysis is designed to **assist** healthcare professionals and should **NOT** replace:
                        - Clinical examination by qualified dermatologists
                        - Histopathological diagnosis (gold standard)
                        - Patient medical history and risk factor assessment
                        - Professional medical judgment
                        
                        **Final diagnosis must always be confirmed through proper clinical evaluation and, when indicated, biopsy with histopathological examination.**
                        
                        ---
                        *Report generated by AI Medical Imaging System | {architecture} Architecture | {dataset} Training Data*
                        """.format(
                            architecture=architecture,
                            dataset=dataset,
                            accuracy=PERFORMANCE_DATA[architecture][dataset]['accuracy'],
                            precision=PERFORMANCE_DATA[architecture][dataset]['precision'],
                            recall=PERFORMANCE_DATA[architecture][dataset]['recall'],
                            auc=PERFORMANCE_DATA[architecture][dataset]['auc']
                        ))
                
                # Add to history button at the bottom
                st.markdown("---")
                if st.button("üíæ Save to History", key="save_single"):
                    result = {
                        'type': 'single',
                        'prediction': pred_label,
                        'confidence': confidence,
                        'timestamp': datetime.now().isoformat(),
                        'model': architecture,
                        'dataset': dataset
                    }
                    st.session_state.analysis_history.append(result)
                    st.success("Analysis saved to history!")
            
            with analysis_tab2:
                st.markdown('<div class="card"><h3>üî• Grad-CAM Visualization</h3></div>', unsafe_allow_html=True)
                
                with st.spinner("Generating Grad-CAM..."):
                    gradcam_img = generate_gradcam(model, input_tensor, image, architecture, target_class=predicted_class)
                
                if gradcam_img is not None:
                    col3, col4 = st.columns(2)
                    with col3:
                        st.image(image.resize((224, 224)), caption="Input Image", use_container_width=True)
                    with col4:
                        st.image(gradcam_img, caption="Attention Map", use_container_width=True)
                    st.info("Red/warm regions indicate areas the model focused on for its prediction.")
                    
                    # Export options
                    col5, col6 = st.columns(2)
                    with col5:
                        if st.button("üì• Download Grad-CAM", key="download_gradcam"):
                            st.markdown(get_image_download_link(Image.fromarray((gradcam_img * 255).astype(np.uint8)), 
                                                              "gradcam.png", "Download Grad-CAM"), unsafe_allow_html=True)
                    with col6:
                        if st.button("üìÑ Export Results", key="export_single"):
                            results = {
                                'prediction': pred_label,
                                'confidence': confidence,
                                'model': architecture,
                                'dataset': dataset,
                                'timestamp': datetime.now().isoformat()
                            }
                            st.download_button(
                                label="üìÑ Download JSON",
                                data=export_results(results, 'json'),
                                file_name="analysis_results.json",
                                mime="application/json",
                                key="download_json"
                            )
            
            with analysis_tab3:
                st.markdown('<div class="card"><h3>üî¨ Visual Features</h3></div>', unsafe_allow_html=True)
                
                with st.spinner("Extracting features..."):
                    features = extract_visual_features(image)
                
                feature_info = [
                    ("‚öñÔ∏è", "Asymmetry", features['asymmetry'], "Left-right symmetry"),
                    ("üî≤", "Border", features['border_irregularity'], "Edge irregularity"),
                    ("üé®", "Color", features['color_variation'], "Color diversity"),
                    ("üß©", "Texture", features['texture_complexity'], "Surface texture"),
                    ("üìè", "Size", features['lesion_size_ratio'], "Lesion coverage")
                ]
                
                st.markdown('<div class="feature-grid">', unsafe_allow_html=True)
                for icon, name, value, desc in feature_info:
                    st.markdown(f"""
                        <div class="feature-item">
                            <div class="icon">{icon}</div>
                            <div class="value">{value:.3f}</div>
                            <div class="label">{name}</div>
                        </div>
                    """, unsafe_allow_html=True)
                st.markdown('</div>', unsafe_allow_html=True)
                
                with st.expander("Detailed Descriptions"):
                    st.markdown("""
                        - **Asymmetry**: Measures left-right symmetry. Higher values indicate more asymmetric lesions.
                        - **Border Irregularity**: Edge detection score. Higher values suggest irregular borders.
                        - **Color Variation**: Standard deviation of RGB channels. Higher values indicate diverse colors.
                        - **Texture Complexity**: Surface texture analysis. Higher values suggest complex textures.
                        - **Lesion Size Ratio**: Proportion of image occupied by lesion.
                    """)
        
        else:
            st.markdown("""
                <div class="upload-card">
                    <h3>üì§ Upload an Image</h3>
                    <p>Drag and drop or click to select a medical image for analysis</p>
                </div>
            """, unsafe_allow_html=True)
    
    with tab2:
        st.markdown('<div class="card"><h3>üìä Model Performance Analytics</h3></div>', unsafe_allow_html=True)
        
        # Performance chart
        fig = get_model_performance_chart(architecture, dataset)
        st.plotly_chart(fig, use_container_width=True)
        
        # Performance metrics table
        st.markdown("### üìà Detailed Metrics")
        perf_data = PERFORMANCE_DATA[architecture][dataset]
        metrics_df = pd.DataFrame({
            'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'],
            'Value': [f"{perf_data['accuracy']:.3f}", f"{perf_data['precision']:.3f}", 
                     f"{perf_data['recall']:.3f}", f"{perf_data['f1']:.3f}", f"{perf_data['auc']:.3f}"]
        })
        st.dataframe(metrics_df, use_container_width=True)
        
        # Performance interpretation
        with st.expander("üí° Performance Interpretation"):
            st.markdown("""
                - **Accuracy**: Overall correctness of predictions
                - **Precision**: True positive rate among predicted positives
                - **Recall**: True positive rate among actual positives  
                - **F1-Score**: Harmonic mean of precision and recall
                - **AUC**: Area under ROC curve (discrimination ability)
                
                **Clinical Context**: Higher precision reduces false positives, higher recall reduces false negatives.
                In medical diagnosis, both are important but context-dependent.
            """)
    
    with tab3:
        st.markdown('<div class="card"><h3>üîÑ Batch Processing</h3></div>', unsafe_allow_html=True)
        
        uploaded_files = st.file_uploader(
            "Upload multiple medical images",
            type=['jpg', 'jpeg', 'png'],
            accept_multiple_files=True,
            help="Select multiple images for batch analysis",
            key="batch_upload"
        )
        
        if uploaded_files:
            st.markdown(f"**{len(uploaded_files)} images uploaded**")
            
            if st.button("üöÄ Process Batch", key="process_batch"):
                images = [Image.open(file) for file in uploaded_files]
                
                with st.spinner("Processing batch..."):
                    batch_results = process_batch_images(images, model, architecture, dataset)
                    st.session_state.batch_results = batch_results
                
                st.success(f"Batch processing completed! {len(batch_results)} images analyzed.")
                
                # Batch summary
                summary = create_batch_summary(batch_results)
                col7, col8, col9 = st.columns(3)
                
                with col7:
                    st.metric("Total Images", summary['total_images'])
                    st.metric("Malignant", summary['malignant_count'])
                
                with col8:
                    st.metric("Benign", summary['benign_count'])
                    st.metric("Avg Confidence", f"{summary['avg_confidence']:.1%}")
                
                with col9:
                    st.metric("Malignant %", f"{summary['malignant_percentage']:.1f}%")
                    st.metric("Benign %", f"{summary['benign_percentage']:.1f}%")
                
                # Results table
                st.markdown("### üìã Batch Results")
                results_df = pd.DataFrame({
                    'Image': [f"Image {i+1}" for i in range(len(batch_results))],
                    'Prediction': [r['prediction'] for r in batch_results],
                    'Confidence': [f"{r['confidence']:.1%}" for r in batch_results]
                })
                st.dataframe(results_df, use_container_width=True)
                
                # Export batch results
                if st.button("üíæ Save Batch Results", key="save_batch"):
                    filename = f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    save_batch_results(batch_results, filename)
                    st.success(f"Results saved to {filename}")
                    
                    # Download link
                    with open(filename, 'r') as f:
                        st.download_button(
                            label="üì• Download Results",
                            data=f.read(),
                            file_name=filename,
                            mime="application/json",
                            key="download_batch"
                        )
    
    with tab4:
        st.markdown('<div class="card"><h3>üìà Model Comparison</h3></div>', unsafe_allow_html=True)
        
        # Comparison table
        comparison_df = create_comparison_table()
        st.dataframe(comparison_df, use_container_width=True)
        
        # Best models by metric
        st.markdown("### üèÜ Top Performers")
        
        col10, col11 = st.columns(2)
        
        with col10:
            st.markdown("**Highest Accuracy**")
            best_acc = comparison_df.loc[comparison_df['Accuracy'].astype(float).idxmax()]
            st.info(f"{best_acc['Model']} ({best_acc['Dataset']}): {best_acc['Accuracy']}")
            
            st.markdown("**Highest AUC**")
            best_auc = comparison_df.loc[comparison_df['AUC'].astype(float).idxmax()]
            st.info(f"{best_auc['Model']} ({best_auc['Dataset']}): {best_auc['AUC']}")
        
        with col11:
            st.markdown("**Highest Precision**")
            best_prec = comparison_df.loc[comparison_df['Precision'].astype(float).idxmax()]
            st.info(f"{best_prec['Model']} ({best_prec['Dataset']}): {best_prec['Precision']}")
            
            st.markdown("**Highest Recall**")
            best_rec = comparison_df.loc[comparison_df['Recall'].astype(float).idxmax()]
            st.info(f"{best_rec['Model']} ({best_rec['Dataset']}): {best_rec['Recall']}")
    
    with tab5:
        st.markdown('<div class="card"><h3>üìã Analysis History</h3></div>', unsafe_allow_html=True)
        
        if st.session_state.analysis_history:
            history_df = pd.DataFrame(st.session_state.analysis_history)
            st.dataframe(history_df, use_container_width=True)
            
            if st.button("üóëÔ∏è Clear History", key="clear_history"):
                st.session_state.analysis_history = []
                st.success("History cleared!")
        else:
            st.info("No analysis history yet. Perform some analyses to see them here.")
    
    # Footer
    st.markdown("""
        <div class="footer">
            <p><strong> Medical Image Classifier </strong> | Powered by Deep Learning & Grad-CAM</p>
            <p>This tool is for research purposes only. Always consult healthcare professionals for medical diagnosis.</p>
        </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()