import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid

import pandas as pd
import numpy as np
from PIL import Image
from pathlib import Path
from tqdm import tqdm
import logging
import os
import shutil
from diffusers import DDPMPipeline, UNet2DModel, DDPMScheduler

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DDPMGenerator128:
    """GÃ©nÃ©rateur DDMP pour images 128x128 Ã  partir du modÃ¨le prÃ©-entraÃ®nÃ© - CORRIGÃ‰"""
    
    def __init__(self, checkpoint_dir, output_dir="./generated_malignant_128"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.output_dir = Path(output_dir)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # CrÃ©er le dossier de sortie
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ParamÃ¨tres du modÃ¨le
        self.image_size = 128  # Taille cible
        self.original_size = 64  # Taille du modÃ¨le entraÃ®nÃ©
        
        logger.info(f"ğŸš€ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
        logger.info(f"ğŸ“ Checkpoints: {self.checkpoint_dir}")
        logger.info(f"ğŸ–¼ï¸ Sortie: {self.output_dir}")
    
    def find_best_checkpoint(self):
        """Trouver le meilleur checkpoint disponible"""
        logger.info("ğŸ” Recherche du meilleur checkpoint...")
        
        # PrioritÃ© 1: Checkpoint "best"
        best_files = [
            "ddpm_pretrained_best.pt",
            "ddpm_best.pt", 
            "ddmp_best.pt"
        ]
        
        for best_file in best_files:
            best_path = self.checkpoint_dir / best_file
            if best_path.exists():
                logger.info(f"âœ… Meilleur checkpoint trouvÃ©: {best_file}")
                return best_path
        
        # PrioritÃ© 2: Checkpoint "latest"
        latest_files = [
            "ddpm_pretrained_latest.pt",
            "ddpm_latest.pt"
        ]
        
        for latest_file in latest_files:
            latest_path = self.checkpoint_dir / latest_file
            if latest_path.exists():
                logger.info(f"âœ… Checkpoint latest trouvÃ©: {latest_file}")
                return latest_path
        
        # PrioritÃ© 3: Checkpoint avec le numÃ©ro d'Ã©poque le plus Ã©levÃ©
        pretrained_files = list(self.checkpoint_dir.glob("ddpm_pretrained_epoch_*.pt"))
        if pretrained_files:
            pretrained_files.sort(key=lambda x: int(x.stem.split('_')[-1]), reverse=True)
            logger.info(f"âœ… Checkpoint prÃ©-entraÃ®nÃ© le plus rÃ©cent: {pretrained_files[0].name}")
            return pretrained_files[0]
        
        # Sinon, chercher les checkpoints normaux
        normal_files = list(self.checkpoint_dir.glob("ddpm_epoch_*.pt"))
        if normal_files:
            normal_files.sort(key=lambda x: int(x.stem.split('_')[-1]), reverse=True)
            logger.info(f"âœ… Checkpoint normal le plus rÃ©cent: {normal_files[0].name}")
            return normal_files[0]
        
        logger.error("âŒ Aucun checkpoint trouvÃ©!")
        return None
    
    def load_model(self, checkpoint_path):
        """Charger le modÃ¨le DDPM depuis un checkpoint"""
        logger.info(f"ğŸ“¥ Chargement du modÃ¨le depuis: {checkpoint_path.name}")
        
        try:
            # Charger le checkpoint avec compatibilitÃ©
            try:
                checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
                logger.info("âœ“ Checkpoint chargÃ© avec weights_only=False")
            except Exception as e:
                logger.warning(f"Tentative avec weights_only=True: {e}")
                
                # Fallback avec safe_globals pour PyTorch 2.6+
                import torch.serialization
                from pathlib import WindowsPath, PosixPath
                
                safe_globals = [
                    WindowsPath, 
                    PosixPath,
                    torch.torch_version.TorchVersion,
                    torch.Size,
                    torch.dtype,
                    torch.device,
                    torch.Tensor,
                ]
                
                with torch.serialization.safe_globals(safe_globals):
                    checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
                    logger.info("âœ“ Checkpoint chargÃ© avec safe_globals")
            
            # VÃ©rifier si c'est un modÃ¨le prÃ©-entraÃ®nÃ© (diffusers) ou custom
            if 'pretrained_model' in checkpoint:
                logger.info("ğŸ¤– ModÃ¨le prÃ©-entraÃ®nÃ© dÃ©tectÃ©")
                return self._load_pretrained_model(checkpoint)
            else:
                logger.info("ğŸ¤– ModÃ¨le custom dÃ©tectÃ©")
                return self._load_custom_model(checkpoint)
                
        except Exception as e:
            logger.error(f"âŒ Erreur lors du chargement: {e}")
            return None, None
    
    def _load_pretrained_model(self, checkpoint):
        """Charger un modÃ¨le prÃ©-entraÃ®nÃ© (diffusers)"""
        try:
            # CrÃ©er le modÃ¨le UNet
            self.unet = UNet2DModel(
                sample_size=64,
                in_channels=3,
                out_channels=3,
                layers_per_block=2,
                block_out_channels=(128, 256, 512, 512),
                down_block_types=(
                    "DownBlock2D",
                    "DownBlock2D", 
                    "AttnDownBlock2D",
                    "DownBlock2D",
                ),
                up_block_types=(
                    "UpBlock2D",
                    "AttnUpBlock2D",
                    "UpBlock2D",
                    "UpBlock2D",
                ),
            ).to(self.device)
            
            # Charger les poids
            self.unet.load_state_dict(checkpoint['unet_state_dict'])
            
            # CrÃ©er le scheduler
            self.scheduler = DDPMScheduler(num_train_timesteps=1000)
            
            # Mode Ã©valuation
            self.unet.eval()
            
            logger.info(f"âœ… ModÃ¨le prÃ©-entraÃ®nÃ© chargÃ© (Ã©poque {checkpoint.get('epoch', '?')})")
            
            return self.unet, self.scheduler
            
        except Exception as e:
            logger.error(f"âŒ Erreur modÃ¨le prÃ©-entraÃ®nÃ©: {e}")
            return None, None
    
    def _load_custom_model(self, checkpoint):
        """Charger un modÃ¨le custom"""
        try:
            logger.warning("âš ï¸ ModÃ¨le custom dÃ©tectÃ© - utilisation d'une architecture par dÃ©faut")
            
            # Architecture par dÃ©faut compatible
            self.unet = UNet2DModel(
                sample_size=64,
                in_channels=3,
                out_channels=3,
                layers_per_block=2,
                block_out_channels=(64, 128, 256, 512),
                down_block_types=(
                    "DownBlock2D",
                    "DownBlock2D", 
                    "DownBlock2D",
                    "DownBlock2D",
                ),
                up_block_types=(
                    "UpBlock2D",
                    "UpBlock2D",
                    "UpBlock2D",
                    "UpBlock2D",
                ),
            ).to(self.device)
            
            # Essayer de charger les poids
            if 'model_state_dict' in checkpoint:
                self.unet.load_state_dict(checkpoint['model_state_dict'])
            elif 'unet_state_dict' in checkpoint:
                self.unet.load_state_dict(checkpoint['unet_state_dict'])
            else:
                # Essayer de charger directement
                self.unet.load_state_dict(checkpoint)
            
            # Scheduler par dÃ©faut
            self.scheduler = DDPMScheduler(num_train_timesteps=1000)
            
            # Mode Ã©valuation
            self.unet.eval()
            
            logger.info(f"âœ… ModÃ¨le custom chargÃ©")
            
            return self.unet, self.scheduler
            
        except Exception as e:
            logger.error(f"âŒ Erreur modÃ¨le custom: {e}")
            return None, None
    
    def setup_upsampler(self):
        """Configurer l'upsampler pour passer de 64x64 Ã  128x128"""
        self.upsampler = nn.Upsample(
            size=(self.image_size, self.image_size), 
            mode='bilinear', 
            align_corners=False
        ).to(self.device)
        
        logger.info(f"âœ… Upsampler configurÃ©: {self.original_size}x{self.original_size} â†’ {self.image_size}x{self.image_size}")
    
    @torch.no_grad()
    def generate_batch(self, batch_size=8, num_inference_steps=50):
        """GÃ©nÃ©rer un batch d'images"""
        self.unet.eval()
        
        # Commencer avec du bruit pur
        shape = (batch_size, 3, self.original_size, self.original_size)
        image = torch.randn(shape, device=self.device)
        
        # Processus de dÃ©bruitage
        self.scheduler.set_timesteps(num_inference_steps)
        
        for t in self.scheduler.timesteps:
            # PrÃ©dire le bruit
            noise_pred = self.unet(image, t).sample
            
            # DÃ©bruiter
            image = self.scheduler.step(noise_pred, t, image).prev_sample
        
        # Upsampler vers 128x128
        if hasattr(self, 'upsampler'):
            image = self.upsampler(image)
        
        return image
    
    def denormalize(self, tensor):
        """DÃ©normaliser les images [-1,1] â†’ [0,1]"""
        return (tensor + 1) / 2
    
    def save_image_pil(self, tensor, path, quality=95):
        """Sauvegarder une image avec PIL pour contrÃ´ler la qualitÃ© - CORRECTION"""
        # Convertir le tensor en PIL Image
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        
        # Assurer que les valeurs sont dans [0, 1]
        tensor = torch.clamp(tensor, 0, 1)
        
        # Convertir en numpy
        np_img = tensor.cpu().numpy()
        np_img = (np_img * 255).astype(np.uint8)
        
        # RÃ©organiser les dimensions (C, H, W) -> (H, W, C)
        if np_img.shape[0] == 3:
            np_img = np_img.transpose(1, 2, 0)
        
        # Convertir en PIL et sauvegarder
        pil_img = Image.fromarray(np_img)
        pil_img.save(path, quality=quality, optimize=True)
    
    def generate_images(self, total_images=1000, batch_size=8, num_inference_steps=50):
        """GÃ©nÃ©rer le nombre total d'images demandÃ© - CORRIGÃ‰"""
        
        logger.info(f"ğŸ¨ GÃ©nÃ©ration de {total_images} images {self.image_size}x{self.image_size}")
        logger.info(f"ğŸ“¦ Batch size: {batch_size}")
        logger.info(f"ğŸ”„ Ã‰tapes de dÃ©bruitage: {num_inference_steps}")
        
        # Calculer le nombre de batches
        num_batches = (total_images + batch_size - 1) // batch_size
        
        # CrÃ©er les dossiers de sortie
        individual_dir = self.output_dir / "individual"
        grids_dir = self.output_dir / "grids"
        individual_dir.mkdir(exist_ok=True)
        grids_dir.mkdir(exist_ok=True)
        
        generated_count = 0
        all_images = []
        
        # GÃ©nÃ©ration par batches
        for batch_idx in tqdm(range(num_batches), desc="GÃ©nÃ©ration d'images"):
            try:
                # Calculer la taille du batch actuel
                remaining = total_images - generated_count
                current_batch_size = min(batch_size, remaining)
                
                if current_batch_size <= 0:
                    break
                
                # GÃ©nÃ©rer le batch
                batch_images = self.generate_batch(current_batch_size, num_inference_steps)
                
                # DÃ©normaliser
                batch_images = self.denormalize(batch_images)
                batch_images = torch.clamp(batch_images, 0, 1)
                
                # Sauvegarder individuellement - CORRECTION: Utiliser save_image au lieu de save_image_pil
                for i, img in enumerate(batch_images):
                    img_idx = generated_count + i + 1
                    
                    # VÃ©rifier la taille
                    if img.shape[-1] != self.image_size or img.shape[-2] != self.image_size:
                        logger.warning(f"âš ï¸ Taille incorrecte: {img.shape}, redimensionnement...")
                        img = F.interpolate(
                            img.unsqueeze(0), 
                            size=(self.image_size, self.image_size), 
                            mode='bilinear', 
                            align_corners=False
                        ).squeeze(0)
                    
                    # CORRECTION: Utiliser save_image sans le paramÃ¨tre quality
                    img_path = individual_dir / f"malignant_ddpm_{img_idx:05d}.png"
                    save_image(img, img_path)
                    
                    # ALTERNATIVE: Utiliser PIL pour qualitÃ© Ã©levÃ©e
                    # self.save_image_pil(img, img_path, quality=95)
                
                # Ajouter Ã  la collection pour les grilles
                all_images.extend(batch_images.cpu())
                generated_count += current_batch_size
                
                # Nettoyer la mÃ©moire pÃ©riodiquement
                if batch_idx % 5 == 0:
                    torch.cuda.empty_cache()
                
                # Statistiques
                if batch_idx % 10 == 0:
                    batch_mean = batch_images.mean().item()
                    batch_std = batch_images.std().item()
                    logger.info(f"  ğŸ“Š Batch {batch_idx}: mean={batch_mean:.3f}, std={batch_std:.3f}")
                
            except Exception as e:
                logger.error(f"âŒ Erreur batch {batch_idx}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # CrÃ©er les grilles d'aperÃ§u
        self._create_preview_grids(all_images[:64], grids_dir)
        
        # Statistiques finales
        logger.info(f"âœ… GÃ©nÃ©ration terminÃ©e!")
        logger.info(f"ğŸ“Š {generated_count} images gÃ©nÃ©rÃ©es")
        logger.info(f"ğŸ“ Images individuelles: {individual_dir}")
        logger.info(f"ğŸ–¼ï¸ Grilles d'aperÃ§u: {grids_dir}")
        
        return generated_count
    
    def _create_preview_grids(self, images, grids_dir):
        """CrÃ©er des grilles d'aperÃ§u - CORRIGÃ‰"""
        if not images:
            return
        
        logger.info("ğŸ–¼ï¸ CrÃ©ation des grilles d'aperÃ§u...")
        
        # Convertir en tenseur si nÃ©cessaire
        if isinstance(images, list):
            images = torch.stack(images[:64])
        
        # DiffÃ©rentes tailles de grilles
        grid_sizes = [16, 36, 64]
        
        for grid_size in grid_sizes:
            if len(images) >= grid_size:
                grid_images = images[:grid_size]
                nrow = int(np.sqrt(grid_size))
                
                # CORRECTION: CrÃ©er la grille sans paramÃ¨tre quality
                grid = make_grid(grid_images, nrow=nrow, padding=2, normalize=False)
                
                # Sauvegarder
                grid_path = grids_dir / f"preview_grid_{grid_size}.png"
                save_image(grid, grid_path)  # CORRECTION: Pas de paramÃ¨tre quality
                
                logger.info(f"  âœ“ Grille {grid_size} sauvegardÃ©e")
    
    def run_generation(self, total_images=1000, batch_size=8, num_inference_steps=50):
        """Pipeline complet de gÃ©nÃ©ration"""
        
        print("ğŸ¥ GÃ‰NÃ‰RATEUR DDPM 128x128")
        print("=" * 50)
        
        try:
            # Ã‰tape 1: Trouver le meilleur checkpoint
            best_checkpoint = self.find_best_checkpoint()
            if best_checkpoint is None:
                logger.error("âŒ Aucun checkpoint trouvÃ©!")
                return False
            
            # Ã‰tape 2: Charger le modÃ¨le
            unet, scheduler = self.load_model(best_checkpoint)
            if unet is None:
                logger.error("âŒ Impossible de charger le modÃ¨le!")
                return False
            
            # Ã‰tape 3: Configurer l'upsampler
            self.setup_upsampler()
            
            # Ã‰tape 4: GÃ©nÃ©rer les images
            generated_count = self.generate_images(
                total_images=total_images,
                batch_size=batch_size, 
                num_inference_steps=num_inference_steps
            )
            
            logger.info(f"ğŸ‰ GÃ©nÃ©ration rÃ©ussie: {generated_count}/{total_images} images")
            return generated_count > 0  # CORRECTION: Retourner True seulement si des images ont Ã©tÃ© gÃ©nÃ©rÃ©es
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la gÃ©nÃ©ration: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        finally:
            # Nettoyer la mÃ©moire
            if hasattr(self, 'unet'):
                del self.unet
            if hasattr(self, 'scheduler'):
                del self.scheduler
            if hasattr(self, 'upsampler'):
                del self.upsampler
            torch.cuda.empty_cache()

def main():
    """Fonction principale"""
    
    # Configuration
    CHECKPOINT_DIR = "./DDPM_pretrained/checkpoints"
    OUTPUT_DIR = "./generated_malignant_ddpm_128"
    TOTAL_IMAGES = 5900
    BATCH_SIZE = 4  # CORRECTION: RÃ©duire pour Ã©viter les erreurs mÃ©moire
    INFERENCE_STEPS = 50
    
    print("ğŸ” Configuration:")
    print(f"ğŸ“ Dossier checkpoints: {CHECKPOINT_DIR}")
    print(f"ğŸ“ Dossier sortie: {OUTPUT_DIR}")
    print(f"ğŸ–¼ï¸ Nombre d'images: {TOTAL_IMAGES}")
    print(f"ğŸ“¦ Batch size: {BATCH_SIZE}")
    print(f"ğŸ”„ Ã‰tapes d'infÃ©rence: {INFERENCE_STEPS}")
    print(f"ğŸ“ Taille finale: 128x128")
    
    # CrÃ©er le gÃ©nÃ©rateur
    generator = DDPMGenerator128(CHECKPOINT_DIR, OUTPUT_DIR)
    
    # Lancer la gÃ©nÃ©ration
    success = generator.run_generation(
        total_images=TOTAL_IMAGES,
        batch_size=BATCH_SIZE,
        num_inference_steps=INFERENCE_STEPS
    )
    
    if success:
        print("\nğŸ‰ GÃ‰NÃ‰RATION TERMINÃ‰E AVEC SUCCÃˆS!")
        print(f"ğŸ“ VÃ©rifiez le dossier: {OUTPUT_DIR}")
        print(f"ğŸ–¼ï¸ Images individuelles: {OUTPUT_DIR}/individual/")
        print(f"ğŸ¯ Grilles d'aperÃ§u: {OUTPUT_DIR}/grids/")
    else:
        print("\nâŒ Ã‰CHEC DE LA GÃ‰NÃ‰RATION!")

if __name__ == "__main__":
    main()