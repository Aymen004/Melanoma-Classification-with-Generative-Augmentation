import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid

import pandas as pd
import numpy as np
from PIL import Image
from pathlib import Path
from tqdm import tqdm
import logging
from huggingface_hub import hf_hub_download
from diffusers import DDPMPipeline, UNet2DModel, DDPMScheduler

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DDPMPretrainedConfig:
    """Configuration pour le DDPM pr√©-entra√Æn√©"""
    def __init__(self):
        # Param√®tres du mod√®le - optimis√©s avec pr√©-entra√Æn√©
        self.image_size = 64  # Compatible avec google/ddpm-ema-imagenet-64
        self.batch_size = 16  # Augment√© gr√¢ce au pr√©-entra√Æn√©
        self.learning_rate = 1e-5  # Garder faible pour fine-tuning
        self.num_epochs = 3000  # Moins d'√©poques n√©cessaires
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Optimisations CUDA
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
        
        # Chemins CORRIG√âS pour √©viter les conflits
        self.data_dir = Path(__file__).parent / "train" / "ISBI2016_ISIC_Part3_Training_Data"
        self.train_csv_dir = Path(__file__).parent / "train"
        self.malignant_csv = self.train_csv_dir / "malignant_images.csv"
        
        # CORRECTION: Utiliser DDPM_pretrained pour √©viter les conflits
        self.checkpoint_dir = Path(__file__).parent / "DDPM_pretrained" / "checkpoints"
        self.output_dir = Path(__file__).parent / "DDPM_pretrained" / "samples"
        
        # Cr√©er les dossiers
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Mod√®le pr√©-entra√Æn√©
        self.pretrained_model = "google/ddpm-ema-imagenet-64"

class MalignantDataset(Dataset):
    """Dataset pour les images malignes - version optimis√©e"""
    def __init__(self, csv_file, data_dir, transform=None):
        self.df = pd.read_csv(csv_file)
        self.data_dir = Path(data_dir)
        self.transform = transform
        
        # V√©rifier les images existantes
        valid_images = []
        logger.info(f"Recherche d'images dans: {self.data_dir}")
        logger.info(f"Nombre d'entr√©es dans le CSV: {len(self.df)}")
        
        # Cr√©er un mapping efficace des fichiers
        image_mapping = {}
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:
            for img_file in self.data_dir.glob(ext):
                stem = img_file.stem
                image_mapping[stem] = img_file
        
        logger.info(f"Fichiers d'images trouv√©s: {len(image_mapping)}")
        
        # Associer les images du CSV
        for _, row in self.df.iterrows():
            image_name = row['image_name']
            if image_name in image_mapping:
                valid_images.append((image_name, str(image_mapping[image_name])))
            else:
                logger.warning(f"Image non trouv√©e: {image_name}")
        
        self.valid_images = valid_images
        logger.info(f"Dataset cr√©√© avec {len(self.valid_images)} images malignes")
    
    def __len__(self):
        return len(self.valid_images)
    
    def __getitem__(self, idx):
        image_name, image_path = self.valid_images[idx]
        
        try:
            image = Image.open(image_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image
        except Exception as e:
            logger.warning(f"Erreur lors du chargement de {image_path}: {e}")
            # Retourner une image vide correctement dimensionn√©e
            return torch.zeros(3, 64, 64)

class DDPMPretrainedTrainer:
    """Trainer pour fine-tuning d'un mod√®le DDPM pr√©-entra√Æn√©"""
    
    def __init__(self, config):
        self.config = config
        self.device = config.device
        
        logger.info("Chargement du mod√®le pr√©-entra√Æn√©...")
        
        try:
            # Charger le pipeline pr√©-entra√Æn√©
            self.pipeline = DDPMPipeline.from_pretrained(
                config.pretrained_model,
                torch_dtype=torch.float32  # Assurer la compatibilit√©
            ).to(self.device)
            
            # Extraire les composants
            self.unet = self.pipeline.unet
            self.scheduler = self.pipeline.scheduler
            
            logger.info(f"‚úì Mod√®le pr√©-entra√Æn√© charg√©: {config.pretrained_model}")
            logger.info(f"‚úì Param√®tres du mod√®le: {sum(p.numel() for p in self.unet.parameters()):,}")
            
        except Exception as e:
            logger.error(f"Erreur lors du chargement du mod√®le pr√©-entra√Æn√©: {e}")
            logger.info("Utilisation d'un mod√®le local...")
            
            # Fallback vers un mod√®le local si le t√©l√©chargement √©choue
            self.unet = UNet2DModel(
                sample_size=64,
                in_channels=3,
                out_channels=3,
                layers_per_block=2,
                block_out_channels=(128, 256, 512, 512),
                down_block_types=(
                    "DownBlock2D",
                    "DownBlock2D", 
                    "AttnDownBlock2D",
                    "DownBlock2D",
                ),
                up_block_types=(
                    "UpBlock2D",
                    "AttnUpBlock2D",
                    "UpBlock2D",
                    "UpBlock2D",
                ),
            ).to(self.device)
            
            self.scheduler = DDPMScheduler(num_train_timesteps=1000)
        
        # Optimiseur pour fine-tuning
        self.optimizer = optim.AdamW(
            self.unet.parameters(),
            lr=config.learning_rate,
            weight_decay=1e-6,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # Scheduler de learning rate
        self.lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.num_epochs,
            eta_min=1e-7
        )
        
        # Mixed precision pour efficacit√©
        try:
            from torch.amp import GradScaler
            self.scaler = GradScaler('cuda')
        except ImportError:
            from torch.cuda.amp import GradScaler
            self.scaler = GradScaler()
    
    def train_step(self, batch):
        """√âtape d'entra√Ænement optimis√©e"""
        batch = batch.to(self.device)
        batch_size = batch.shape[0]
        
        # √âchantillonner des timesteps al√©atoires
        timesteps = torch.randint(
            0, self.scheduler.config.num_train_timesteps, 
            (batch_size,), device=self.device
        ).long()
        
        # Ajouter du bruit aux images
        noise = torch.randn_like(batch)
        noisy_images = self.scheduler.add_noise(batch, noise, timesteps)
        
        # Pr√©diction avec mixed precision
        try:
            from torch.amp import autocast
            autocast_context = autocast('cuda')
        except ImportError:
            from torch.cuda.amp import autocast
            autocast_context = autocast()
        
        with autocast_context:
            # Pr√©dire le bruit
            noise_pred = self.unet(noisy_images, timesteps).sample
            
            # Calculer la loss
            loss = F.mse_loss(noise_pred, noise)
            
            # V√©rifier la stabilit√©
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning("Loss NaN d√©tect√©e, skipping batch")
                return 0.0
        
        # Backward pass avec gradient scaling
        self.optimizer.zero_grad()
        self.scaler.scale(loss).backward()
        
        # Gradient clipping
        self.scaler.unscale_(self.optimizer)
        torch.nn.utils.clip_grad_norm_(self.unet.parameters(), max_norm=1.0)
        
        self.scaler.step(self.optimizer)
        self.scaler.update()
        
        return loss.item()
    
    @torch.no_grad()
    def generate_samples(self, num_samples=16):
        """G√©n√©ration d'√©chantillons"""
        self.unet.eval()
        
        # Commencer avec du bruit pur
        shape = (num_samples, 3, self.config.image_size, self.config.image_size)
        image = torch.randn(shape, device=self.device)
        
        # Processus de d√©bruitage
        self.scheduler.set_timesteps(50)  # Moins d'√©tapes pour plus de rapidit√©
        
        for t in tqdm(self.scheduler.timesteps, desc="G√©n√©ration"):
            # Pr√©dire le bruit
            noise_pred = self.unet(image, t).sample
            
            # D√©bruiter
            image = self.scheduler.step(noise_pred, t, image).prev_sample
        
        return image
    
    def save_checkpoint(self, epoch, loss, is_best=False):
        """Sauvegarder un checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'unet_state_dict': self.unet.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'lr_scheduler_state_dict': self.lr_scheduler.state_dict(),
            'scaler_state_dict': self.scaler.state_dict(),
            'loss': loss,
            'config': self.config.__dict__,
            'pretrained_model': self.config.pretrained_model
        }
        
        # Sauvegarder checkpoint normal
        checkpoint_path = self.config.checkpoint_dir / f"ddpm_pretrained_epoch_{epoch}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # Sauvegarder checkpoint "latest"
        latest_path = self.config.checkpoint_dir / "ddpm_pretrained_latest.pt"
        torch.save(checkpoint, latest_path)
        
        # Sauvegarder meilleur mod√®le
        if is_best:
            best_path = self.config.checkpoint_dir / "ddpm_pretrained_best.pt"
            torch.save(checkpoint, best_path)
        
        logger.info(f"Checkpoint sauvegard√©: {checkpoint_path}")
    
    def load_checkpoint(self, checkpoint_path):
        """Charger un checkpoint"""
        checkpoint_path = Path(checkpoint_path)
        
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpoint non trouv√©: {checkpoint_path}")
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        except Exception as e:
            logger.warning(f"Chargement avec weights_only=False √©chou√©: {e}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
        
        # Charger les √©tats
        self.unet.load_state_dict(checkpoint['unet_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if 'lr_scheduler_state_dict' in checkpoint:
            self.lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])
        
        if 'scaler_state_dict' in checkpoint:
            try:
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
            except Exception as e:
                logger.warning(f"Impossible de charger scaler state: {e}")
        
        logger.info(f"Checkpoint charg√©: {checkpoint_path}")
        return checkpoint['epoch'], checkpoint['loss']
    
    def find_latest_checkpoint(self):
        """Trouver le checkpoint le plus r√©cent"""
        if not self.config.checkpoint_dir.exists():
            return None
        
        # Chercher le checkpoint "latest"
        latest_path = self.config.checkpoint_dir / "ddpm_pretrained_latest.pt"
        if latest_path.exists():
            return latest_path
        
        # Sinon, chercher par num√©ro d'√©poque
        checkpoint_files = list(self.config.checkpoint_dir.glob("ddpm_pretrained_epoch_*.pt"))
        if not checkpoint_files:
            return None
        
        checkpoint_files.sort(key=lambda x: int(x.stem.split('_')[-1]))
        return checkpoint_files[-1]

def create_transforms(image_size):
    """Transformations optimis√©es pour le pr√©-entra√Æn√©"""
    return transforms.Compose([
        transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BILINEAR),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # [-1, 1]
    ])

def denormalize(tensor):
    """D√©normaliser les images pour visualisation"""
    return (tensor + 1) / 2  # [-1,1] -> [0,1]

def visualize_samples(trainer, epoch, num_samples=16):
    """G√©n√©rer et sauvegarder des √©chantillons"""
    logger.info(f"üé® G√©n√©ration d'√©chantillons √† l'√©poque {epoch}...")
    
    try:
        # G√©n√©rer les √©chantillons
        samples = trainer.generate_samples(num_samples)
        
        # D√©normaliser
        samples = denormalize(samples)
        samples = torch.clamp(samples, 0, 1)
        
        # Statistiques
        sample_std = samples.std().item()
        sample_mean = samples.mean().item()
        
        logger.info(f"  üìä Mean: {sample_mean:.3f}, Std: {sample_std:.3f}")
        
        if sample_std > 0.4:
            logger.warning(f"  ‚ö†Ô∏è  Qualit√© m√©diocre (std={sample_std:.3f})")
        else:
            logger.info(f"  ‚úì Qualit√© acceptable (std={sample_std:.3f})")
        
        # Sauvegarder grille
        grid = make_grid(samples, nrow=4, padding=2)
        save_path = trainer.config.output_dir / f"samples_epoch_{epoch}.png"
        save_image(grid, save_path)
        
        # Sauvegarder √©chantillons individuels
        individual_dir = trainer.config.output_dir / f"individual_epoch_{epoch}"
        individual_dir.mkdir(exist_ok=True)
        
        for i, sample in enumerate(samples):
            individual_path = individual_dir / f"sample_{i:03d}.png"
            save_image(sample, individual_path)
        
        logger.info(f"‚úì √âchantillons sauvegard√©s: {save_path}")
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration: {e}")

def main():
    """Fonction principale avec mod√®le pr√©-entra√Æn√©"""
    # Configuration
    config = DDPMPretrainedConfig()
    
    logger.info(f"üöÄ DDPM Fine-tuning avec mod√®le pr√©-entra√Æn√©")
    logger.info(f"Device: {config.device}")
    logger.info(f"Mod√®le pr√©-entra√Æn√©: {config.pretrained_model}")
    logger.info(f"Batch size: {config.batch_size}")
    logger.info(f"Learning rate: {config.learning_rate}")
    
    # V√©rifications des chemins
    logger.info(f"üìÅ Checkpoints: {config.checkpoint_dir}")
    logger.info(f"üñºÔ∏è Samples: {config.output_dir}")
    logger.info(f"üìä Dataset: {config.data_dir}")
    
    # Dataset
    transform = create_transforms(config.image_size)
    dataset = MalignantDataset(config.malignant_csv, config.data_dir, transform=transform)
    
    if len(dataset) == 0:
        logger.error("‚ùå Aucune image trouv√©e dans le dataset!")
        return
    
    logger.info(f"‚úì Dataset charg√©: {len(dataset)} images malignes")
    
    # DataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=2,
        pin_memory=True,
        drop_last=True
    )
    
    # V√©rification des donn√©es
    sample_batch = next(iter(dataloader))
    logger.info(f"‚úì Donn√©es v√©rifi√©es: shape={sample_batch.shape}, range=[{sample_batch.min():.3f}, {sample_batch.max():.3f}]")
    
    # Sauvegarder √©chantillons du dataset
    dataset_samples = denormalize(sample_batch[:4])
    dataset_samples = torch.clamp(dataset_samples, 0, 1)
    save_image(dataset_samples, config.output_dir / "dataset_samples.png", nrow=2)
    logger.info(f"‚úì √âchantillons du dataset sauvegard√©s")
    
    # Initialiser le trainer
    trainer = DDPMPretrainedTrainer(config)
    
    # Recherche de checkpoint existant
    start_epoch = 0
    best_loss = float('inf')
    
    latest_checkpoint = trainer.find_latest_checkpoint()
    if latest_checkpoint:
        try:
            start_epoch, last_loss = trainer.load_checkpoint(latest_checkpoint)
            best_loss = last_loss
            logger.info(f"‚úÖ Checkpoint charg√©: √©poque {start_epoch}, loss {last_loss:.6f}")
        except Exception as e:
            logger.warning(f"‚ùå Erreur checkpoint: {e}")
            start_epoch = 0
            best_loss = float('inf')
    
    # Boucle d'entra√Ænement
    logger.info(f"üéØ D√©but de l'entra√Ænement: {start_epoch} ‚Üí {config.num_epochs}")
    
    for epoch in range(start_epoch, config.num_epochs):
        trainer.unet.train()
        epoch_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{config.num_epochs}")
        
        for batch in progress_bar:
            loss = trainer.train_step(batch)
            
            if loss > 0:
                epoch_loss += loss
                num_batches += 1
            
            # Mise √† jour de la barre de progression
            current_lr = trainer.optimizer.param_groups[0]['lr']
            progress_bar.set_postfix({
                'Loss': f'{loss:.4f}',
                'Best': f'{best_loss:.4f}',
                'LR': f'{current_lr:.2e}'
            })
        
        # Mise √† jour du learning rate
        trainer.lr_scheduler.step()
        
        if num_batches > 0:
            avg_loss = epoch_loss / num_batches
            
            # V√©rifier si c'est le meilleur mod√®le
            is_best = avg_loss < best_loss
            if is_best:
                best_loss = avg_loss
                logger.info(f"üèÜ Nouveau meilleur mod√®le! Loss: {avg_loss:.6f}")
        
        # Sauvegardes p√©riodiques
        if (epoch + 1) % 25 == 0 or is_best:
            trainer.save_checkpoint(epoch + 1, avg_loss, is_best=is_best)
            logger.info(f"üíæ Checkpoint sauvegard√© √† l'√©poque {epoch + 1}")
        
        # G√©n√©ration d'√©chantillons
        if (epoch + 1) % 25 == 0:
            visualize_samples(trainer, epoch + 1, num_samples=16)
    
    # G√©n√©ration finale
    logger.info("üé® G√©n√©ration d'√©chantillons finaux...")
    try:
        # Diff√©rentes tailles d'√©chantillons
        for num_samples in [4, 16, 64]:
            samples = trainer.generate_samples(num_samples)
            samples = denormalize(samples)
            samples = torch.clamp(samples, 0, 1)
            
            # Grille
            nrow = 2 if num_samples == 4 else (4 if num_samples == 16 else 8)
            grid = make_grid(samples, nrow=nrow, padding=2)
            save_path = config.output_dir / f"final_generation_{num_samples}.png"
            save_image(grid, save_path)
            
            logger.info(f"‚úì {num_samples} √©chantillons finaux sauvegard√©s")
        
    except Exception as e:
        logger.error(f"Erreur g√©n√©ration finale: {e}")
    
    logger.info("üèÅ Entra√Ænement termin√©!")
    logger.info(f"üìä Meilleure loss: {best_loss:.6f}")
    logger.info(f"üìÅ Checkpoints: {config.checkpoint_dir}")
    logger.info(f"üñºÔ∏è √âchantillons: {config.output_dir}")

if __name__ == "__main__":
    main()
